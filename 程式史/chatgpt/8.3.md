## 8.3 Attention 注意力機制

Attention 注意力機制是一種在深度學習中常用的機制，用於處理序列型數據，例如自然語言處理中的句子或音頻中的時間序列數據。這種機制被廣泛應用於機器翻譯、語音識別、圖像處理等任務中。

在傳統的序列模型中，例如循環神經網絡(RNN)和長短期記憶(LSTM)，網絡在處理序列數據時須考慮到上下文相關性，但不同位置的上下文重要性是不同的。然而傳統的序列模型通常具有固定大小的狀態，無法自適應地對不同的上下文給予不同的重要性。這時候，Attention 機制就發揮了作用。

Attention 可以理解為一個權重分配機制，在序列中對於不同位置的重要性進行動態調整，使模型能夠專注於關鍵的部分。它可以在不同模型架構和任務中實現，例如 Seq2Seq 模型、Transformer 等。這種機制一般由三個關鍵的部分組成：Query、Key 和 Value。

首先，模型將一個輸入序列轉換為 Query、Key 和 Value。Query 是用於指示關注哪些部分的向量，Key 和 Value 則是用於儲存序列信息的向量。通常情況下，Query 的維度和 Key 的維度是相同的。

然後，計算 Query 與 Key 之間的相似度。常用的計算相似度的方法有點積、線性映射和神經網絡等。這一步的目的是找出 Query 和 Key 之間的關聯度，以便對序列進行加權。

接著，將相似度進行規模化，可以使用 softmax 函數實現。這樣可以保證不同位置的權重加總為1，也就是給不同位置的上下文分配合適的權重。

最後，使用加權後的 Value 以某種方式進行聚合，常見的方法有加權平均和加權求和。聚合後的結果就是具有注意力機制的輸出。

總結來說，Attention 注意力機制是一種能夠自適應調整不同位置上下文重要性的機制。這種機制在處理序列型數據時被廣泛應用，能夠提高模型的性能和對其背後模式的理解。